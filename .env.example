OPENAI_API_KEY=sk-...

# New: LLM Provider Switch
# Options: 'openai' (Default) or 'ollama' (Local Llama 3)
LLM_PROVIDER=openai

# Ollama Configuration
# Local (Windows/Mac): http://host.docker.internal:11434
# Local (Linux): http://localhost:11434
# Remote Server: http://<YOUR_SERVER_IP>:11434
OLLAMA_BASE_URL=http://host.docker.internal:11434

# Ollama Models (Default: llama3.2 and nomic-embed-text)
# For Server Match: Use 'llama3' and 'mxbai-embed-large'
OLLAMA_MODEL=llama3.2
OLLAMA_EMBED_MODEL=nomic-embed-text
